{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-11T22:22:29.230578Z",
     "start_time": "2024-10-11T22:22:29.221578Z"
    }
   },
   "source": [
    "\n",
    "HTML_INPUT_PATH = 'html/' # path to html-files and their corresponding folders to be converted into plain-text\n",
    "CSV_OUTPUT_PATH = \"out/\" # top-level directory for all sort of output files; The CSV-file can be used to get all gathered information in one single file to make statistics easier to implement.\n",
    "PLAIN_OUTPUT_PATH = 'out/plain/' # directory for exported plain-text per file\n",
    "JSON_OUTPUT_PATH = \"out/json/\" # directory for exported json-files\n",
    "CCS_INPUT_PATH = \"CCS_Classes.json\" # path to ccs_Classes.json file\n",
    "\n",
    "\n",
    "class CleanedPaper: # structure to store all information about a fully extracted paper\n",
    "    def __init__(self, filename: str, doi: str, authors: str, year: int, title: str, keywords: str, ccs: str,\n",
    "                 specified_class: str):\n",
    "        self.filename = filename\n",
    "        self.doi = doi\n",
    "        self.authors = authors\n",
    "        self.year = year\n",
    "        self.title = title\n",
    "        self.keywords = keywords\n",
    "        self.ccs = ccs\n",
    "        self.specified_class = specified_class\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "try:  #catch errors corresponding to empty or not existing directories for HTML input\n",
    "    if len(os.listdir(HTML_INPUT_PATH)) == 0:\n",
    "        print(\"No HTML input files found!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No Directory Found for HTML Input! Please change path in HTML_INPUT_PATH: \" + HTML_INPUT_PATH)\n",
    "\n",
    "try: # catch errors corresponding with reading the ccs_classes from ccs_Classes.json\n",
    "    f = open(CCS_INPUT_PATH, 'r')\n",
    "    json_ccs = json.load(f) # load input from file as json structure\n",
    "    CCS_TOP_CLASSES = json_ccs['top_classes']\n",
    "    CCS_SUB_GENERAL_AND_REFERENCE = json_ccs['sub_general_and_reference']\n",
    "    CCS_SUB_HARDWARE = json_ccs['sub_hardware']\n",
    "    CCS_SUB_COMPUTER_SYSTEMS_ORGANIZATION = json_ccs['sub_computer_systems_organization']\n",
    "    CCS_SUB_NETWORKS = json_ccs['sub_networks']\n",
    "    CCS_SUB_SOFTWARE_AND_ITS_ENGINEERING = json_ccs['sub_software_and_its_engineering']\n",
    "    CCS_SUB_THEORY_OF_COMPUTING = json_ccs['sub_theory_of_computing']\n",
    "    CCS_SUB_MATHEMATICS_OF_COMPUTING = json_ccs['sub_mathematics_of_computing']\n",
    "    CCS_SUB_INFORMATION_SYSTEMS = json_ccs['sub_information_systems']\n",
    "    CCS_SUB_SECURITY_AND_PRIVACY = json_ccs['sub_security_and_privacy']\n",
    "    CCS_SUB_HUMAN_CENTERED_COMPUTING = json_ccs['sub_human_centered_computing']\n",
    "    CCS_SUB_COMPUTING_METHODOLOGIES = json_ccs['sub_computing_methodologies']\n",
    "    CCS_SUB_APPLIED_COMPUTING = json_ccs['sub_applied_computing']\n",
    "    CCS_SUB_SOCIAL_AND_PROFESSIONAL_TOPICS = json_ccs['sub_social_and_professional_topics']\n",
    "    \n",
    "    sum_classes = len(CCS_TOP_CLASSES)+len(CCS_SUB_GENERAL_AND_REFERENCE)+len(CCS_SUB_HARDWARE)+len(CCS_SUB_COMPUTER_SYSTEMS_ORGANIZATION)+len(CCS_SUB_NETWORKS)+len(CCS_SUB_SOFTWARE_AND_ITS_ENGINEERING)+len(CCS_SUB_THEORY_OF_COMPUTING)+len(CCS_SUB_MATHEMATICS_OF_COMPUTING)+len(CCS_SUB_INFORMATION_SYSTEMS)+len(CCS_SUB_SECURITY_AND_PRIVACY)+len(CCS_SUB_HUMAN_CENTERED_COMPUTING)+len(CCS_SUB_COMPUTING_METHODOLOGIES)+len(CCS_SUB_APPLIED_COMPUTING)+len(CCS_SUB_SOCIAL_AND_PROFESSIONAL_TOPICS) # sum up how many classes are found overall        \n",
    "    print(\"classes found: \",sum_classes)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No file Found for CCS Input!\")\n",
    "try:  #catch errors corresponding to empty or not existing directories for plaintext output\n",
    "    os.listdir(CSV_OUTPUT_PATH)\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(CSV_OUTPUT_PATH)\n",
    "    print(\"No Directory Found for csv output! Created this directory in CSV_OUTPUT_PATH: \" + CSV_OUTPUT_PATH)\n",
    "try:  #catch errors corresponding to empty or not existing directories for plaintext output\n",
    "    os.listdir(PLAIN_OUTPUT_PATH)\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(PLAIN_OUTPUT_PATH)\n",
    "    print(\"No Directory Found for plaintext output! Created this directory in PLAIN_OUTPUT_PATH: \" + PLAIN_OUTPUT_PATH)\n",
    "try:  #catch errors corresponding to empty or not existing directories for json output\n",
    "    os.listdir(JSON_OUTPUT_PATH)\n",
    "except FileNotFoundError:\n",
    "    os.mkdir(JSON_OUTPUT_PATH)\n",
    "    print(\"No Directory Found for json output! Created this directory in JSON_OUTPUT_PATH: \" + JSON_OUTPUT_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "classes found:  2092\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7e4fe7fb-04d5-4188-92f9-9437d5e8dc7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T22:24:12.901676Z",
     "start_time": "2024-10-11T22:24:12.627812Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "all_CCS = list()\n",
    "all_CCS.extend(CCS_TOP_CLASSES)\n",
    "all_CCS.extend(CCS_SUB_GENERAL_AND_REFERENCE)\n",
    "all_CCS.extend(CCS_SUB_HARDWARE)\n",
    "all_CCS.extend(CCS_SUB_COMPUTER_SYSTEMS_ORGANIZATION)\n",
    "all_CCS.extend(CCS_SUB_NETWORKS)\n",
    "all_CCS.extend(CCS_SUB_SOFTWARE_AND_ITS_ENGINEERING)\n",
    "all_CCS.extend(CCS_SUB_THEORY_OF_COMPUTING)\n",
    "all_CCS.extend(CCS_SUB_MATHEMATICS_OF_COMPUTING)\n",
    "all_CCS.extend(CCS_SUB_INFORMATION_SYSTEMS)\n",
    "all_CCS.extend(CCS_SUB_SECURITY_AND_PRIVACY)\n",
    "all_CCS.extend(CCS_SUB_HUMAN_CENTERED_COMPUTING)\n",
    "all_CCS.extend(CCS_SUB_COMPUTING_METHODOLOGIES)\n",
    "all_CCS.extend(CCS_SUB_APPLIED_COMPUTING)\n",
    "all_CCS.extend(CCS_SUB_SOCIAL_AND_PROFESSIONAL_TOPICS)\n",
    "df = pd.DataFrame(all_CCS, columns=[\"Col\"])\n",
    "print(len(df.Col)) # number of categories overall\n",
    "print(len(df.Col.unique())) # number of unique categories\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n",
      "1910\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:38:45.037204Z",
     "start_time": "2024-10-07T16:38:45.002200Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(CCS_TOP_CLASSES)+len(CCS_SUB_GENERAL_AND_REFERENCE)+len(CCS_SUB_HUMAN_CENTERED_COMPUTING)+len(CCS_SUB_APPLIED_COMPUTING)) # count number of selected ccs categories",
   "id": "9a8463d8c0d4c03d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "782e695c3553e21d",
   "metadata": {},
   "source": [
    "extract plaintext from all html files"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2649d94e24af09a",
   "metadata": {},
   "source": [
    "import html2text\n",
    "import os\n",
    "\n",
    "# extract Content of html files into plain txt format for easier handling and content extraction\n",
    "\n",
    "num = 0 # counter for readable outputs\n",
    "for file in os.listdir(HTML_INPUT_PATH): \n",
    "    if file.endswith('.html'): # loop through all files ending with .html to write them as plaintext\n",
    "        print(\"progressing: writing plaintext files from html documents: file \" + file + \" \" + str(\n",
    "            num + 1) + \" of \" + str(len(os.listdir(HTML_INPUT_PATH)) / 2) + \" (\" + str(round(\n",
    "            (num + 1) / (len(os.listdir(HTML_INPUT_PATH)) / 2) * 100, 2)) + \"%)\", end='\\r')\n",
    "        html_content = open(HTML_INPUT_PATH + file, encoding=\"utf-8\").read() # open html file with utf-8 encoding\n",
    "        num += 1 # set counter\n",
    "\n",
    "        html_converter = html2text.HTML2Text() # initialize html2text converter with bundle of options listed below\n",
    "        html_converter.ignore_links = True\n",
    "        html_converter.ignore_images = True\n",
    "        html_converter.images_to_alt = True\n",
    "        html_converter.ignore_tables = True\n",
    "        html_converter.ignore_mailto_links = True\n",
    "        html_converter.skip_internal_links = True\n",
    "        html_converter.use_automatic_links = False\n",
    "        html_converter.body_width = 0 # no body_width so no inserted linebreaks\n",
    "        html_converter.white_space_trim = True\n",
    "\n",
    "        text = html_converter.handle(html_content) # convert plaintext from html-file\n",
    "        output_txt = open(PLAIN_OUTPUT_PATH + \"plain_text__\" + file.split(\".\")[0] + \".txt\", \"w\", encoding=\"utf-8\") # write converted plaintext into .txt file\n",
    "        output_txt.write(text) \n",
    "        output_txt.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cf93b6cc7119a14",
   "metadata": {},
   "source": [
    "Extract content from plain txt_files\n",
    "* DOI\n",
    "* Authors\n",
    "* Year\n",
    "* Title\n",
    "* Keywords\n",
    "* CCS\n",
    "* Specified_CCS"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac3b087bcee552c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T22:25:26.762363Z",
     "start_time": "2024-10-11T22:25:26.731528Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_paper(path):\n",
    "    f = open(path, 'r', encoding=\"utf-8\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # extract title from plaintext via first '\\n' identification\n",
    "    title = text.split(\"\\n\\n\")[0]\n",
    "    title = title[2:len(title) - 1].replace(\"\\n\", \" \")  # remove leading '#' and trailing '\\n' in title\n",
    "\n",
    "    # extract doi from plaintext\n",
    "    try:\n",
    "        doi = re.split(\"DOI:\", text, flags=re.IGNORECASE)[1]  # split after \"DOI:\" until next '\\n'\n",
    "        doi = doi.split(\"\\n\")[0].strip()\n",
    "    except IndexError:\n",
    "        doi = \"undefined\"\n",
    "\n",
    "    # extract authors as list from plaintext\n",
    "    authors = list()\n",
    "    tmp_authors = re.split(\"DOI\", text, flags=re.IGNORECASE)[0] # split before \"DOI\"\n",
    "    tmp_authors = tmp_authors.split(\"\\n\\n\")[1:] # split after first complete empty line (\\n\\n)\n",
    "    for author in tmp_authors:\n",
    "        author = author.split(\",\")[0].strip()\n",
    "        if len(author) > 2:\n",
    "            authors.append(author) # extract authors name from whole author-information\n",
    "    authors = \";\".join(authors)\n",
    "\n",
    "\n",
    "    # extract year of publication from plain text\n",
    "    year = text.split(\"DOI\")[1].split(\"\\n\")[1].strip().split(\",\")[-1].split(\" \")[-1].strip()\n",
    "    try:\n",
    "        year = int(year)\n",
    "    except ValueError: # if no year could be extracted, define year as -1\n",
    "        year = int(-1)\n",
    "\n",
    "    # extract given full CCS classification tree from plaintext\n",
    "    try:\n",
    "        ccs = re.split(\"CCS CONCEPTS:\", text, flags=re.IGNORECASE)[1]  # split after \"CCS concepts:\" until next '\\n'\n",
    "        ccs = ccs.split(\"\\n\")[0]\n",
    "    except IndexError:\n",
    "        ccs = \"undefined\"\n",
    "\n",
    "    # extract most important CCS classification branch from given ccs concepts\n",
    "    specified_class = []  #default fallback to have the most general classification at least \n",
    "    # specified_class extraction in own text\n",
    "\n",
    "    ccs_splitted = ccs.lower().split(\";\") # split given ccs Classification from Plaintext\n",
    "    for ccs_split in ccs_splitted:\n",
    "        for given in CCS_TOP_CLASSES:\n",
    "            if given in ccs_split:\n",
    "                tmp = given\n",
    "                if 'general and reference' in given: # extract more precise classifications from class \"general and reference\"\n",
    "                    for detailed_gr_class in CCS_SUB_GENERAL_AND_REFERENCE:\n",
    "                        if detailed_gr_class in ccs_split:\n",
    "                            tmp = detailed_gr_class\n",
    "                            if not tmp in specified_class:\n",
    "                                specified_class.append(tmp)\n",
    "                if 'human-centered computing' in given: # extract more precise classifications from class \"human-centered computing\"\n",
    "                    for detailed_hcc_class in CCS_SUB_HUMAN_CENTERED_COMPUTING:\n",
    "                        if detailed_hcc_class in ccs_split:\n",
    "                            tmp = detailed_hcc_class\n",
    "                            if not tmp in specified_class:\n",
    "                                specified_class.append(tmp)\n",
    "                if 'applied computing' in given: # extract more precise classifications from class \"applied computing\"\n",
    "                    for detailed_ac_class in CCS_SUB_APPLIED_COMPUTING:\n",
    "                        if detailed_ac_class in ccs_split:\n",
    "                            tmp = detailed_ac_class\n",
    "                            if not tmp in specified_class:\n",
    "                                specified_class.append(tmp)\n",
    "\n",
    "                if not given in specified_class: # extract all general top-level classifications\n",
    "                    specified_class.append(given)\n",
    "\n",
    "    specified_class = \";\".join(specified_class) # join list of given classes as string seperated by semicolon\n",
    "\n",
    "    # extract given keywords from plaintext\n",
    "    try:\n",
    "        keywords = re.split(\"KEYWORDS:\", text, flags=re.IGNORECASE)[1]  # split after \"Keywords:\" until next '\\n'\n",
    "        keywords = keywords.split(\"\\n\")[0]\n",
    "        keywords = re.sub(r'\\s*[;,]\\s*', ',', keywords)\n",
    "        keywords = keywords.split(\",\")\n",
    "    except IndexError: # if no keywords are find, define them as \"undefined\"\n",
    "        keywords = \"undefined\"\n",
    "\n",
    "    return CleanedPaper(filename=path, doi=doi, authors=authors, year=year, title=title, keywords=keywords, ccs=ccs,\n",
    "                        specified_class=specified_class) # return cleanedPaper instance with all fields setted\n",
    "\n",
    "# print(clean_paper(PLAIN_OUTPUT_PATH+\"plain_text__3300237.txt\").authors)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "e8e4ec780636e000",
   "metadata": {},
   "source": [
    "create clean_paper instance of all given plaintext files to fill JSON_file per file with all information.\n",
    "Also export them into single csv file for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc4abd1d8117a60d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:19:54.230555Z",
     "start_time": "2024-08-19T11:19:22.733097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing json_files -> Progress: 100.0%  3724 of 3724 is processing...  [plain_text__3582074.txt]\r\n",
      "CSV file exported to [out/cleaned_paper.csv] found 143 errors. See list of all errors in out/errors.csv]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(columns=['filename', 'doi', 'authors', 'title','year', 'keywords', 'ccs', 'specified_class']) # columns to be written into the main data.csv file\n",
    "cnt = 1\n",
    "errors = []\n",
    "\n",
    "for file in os.listdir(PLAIN_OUTPUT_PATH):\n",
    "    if file.endswith('.txt'):\n",
    "        print(\"Writing json_files -> Progress: \" + str(\n",
    "            round((cnt / len(os.listdir(PLAIN_OUTPUT_PATH))) * 100, 1)) + \"%  \" + str(cnt) + \" of \" + str(\n",
    "            len(os.listdir(PLAIN_OUTPUT_PATH))) + \" is processing...  [\" + file + \"]\", end='\\r')\n",
    "\n",
    "        result = clean_paper(os.path.join(PLAIN_OUTPUT_PATH, file))\n",
    "        if not result.ccs == \"undefined\":  #if paper contains \"undefined\" in ccs column, it is most likely not in the correct format and therefor not usable. it is logged as error /corrupted file\n",
    "            example = open('format_output.json', 'r',\n",
    "                           encoding=\"utf-8\")  #open example json-file and write single files for every document\n",
    "            json_data = json.load(example)\n",
    "            example.close()\n",
    "\n",
    "            # set all fields with information from cleanedPaper named result\n",
    "            json_data['title'] = result.title\n",
    "            json_data['topics'] = result.specified_class.split(\";\")\n",
    "            json_data['authors'] = result.authors.split(\";\")\n",
    "            json_data['year'] = result.year\n",
    "            if (result.doi.__contains__(\"doi.org/\")): # extract and build doi-links if necessary\n",
    "                json_data['doi'] = result.doi.split(\"doi.org/\")[1]\n",
    "                json_data['link'] = result.doi\n",
    "            else:\n",
    "                json_data['doi'] = result.doi\n",
    "                json_data['link'] = \"https://doi.org/\" + result.doi\n",
    "\n",
    "            out = open(JSON_OUTPUT_PATH + file.split(\".\")[0].replace(\"plain_text__\", '') + \"_quant_data.json\", \"w\",\n",
    "                       encoding=\"utf-8\") # write json-file for each cleanedPaper instance\n",
    "            json.dump(json_data, out, indent=4)\n",
    "            out.close()\n",
    "\n",
    "            data.loc[len(data.index)] = file.split(\".\")[\n",
    "                0], result.doi, result.authors, result.title, result.year, result.keywords, result.ccs, result.specified_class\n",
    "        else:  #sum up all error files to be logged in csv later\n",
    "            errors.append(file)\n",
    "        cnt += 1\n",
    "\n",
    "data.to_csv(CSV_OUTPUT_PATH + \"cleaned_paper.csv\", index=False) # save all cleanedPaper instance as one csv-file\n",
    "errors = pd.DataFrame(errors) # extract all names of corrupted papers\n",
    "errors.to_csv(CSV_OUTPUT_PATH + \"errors.csv\", index=False)\n",
    "print(\"\\nCSV file exported to [\" + CSV_OUTPUT_PATH + \"cleaned_paper.csv]\" + \" found \" + str(\n",
    "    len(errors)) + \" errors. See list of all errors in \" + CSV_OUTPUT_PATH + \"errors.csv]\") # output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8449c13a44a4864",
   "metadata": {},
   "source": [
    "**ONLY UNCOMMENT IF YOU WANT TO REMOVE THE PLAINTEXT DIR!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcff144d86601d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T16:14:22.697313Z",
     "start_time": "2024-07-31T16:14:22.694988Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#shutil.rmtree(PLAIN_OUTPUT_PATH)\n",
    "#print(\"plaintext directory has been removed to free memory!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
